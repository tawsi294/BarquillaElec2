{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w9u5rHiD1EQ"
      },
      "outputs": [],
      "source": [
        "# Clean up any previous Spark installations\n",
        "!rm -rf spark-3.4.1-bin-hadoop3 spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java (required for Spark)\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Spark\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Confirm the file exists\n",
        "!ls -lh spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Extract Spark\n",
        "!tar -xzf spark-3.4.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "27Wd69Wr8YZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9e7d85-0edc-4af6-d1e6-b76de996c996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-07 13:23:05--  https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 388341449 (370M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.4.1-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.4.1-bin-had 100%[===================>] 370.35M  6.87MB/s    in 98s     \n",
            "\n",
            "2025-04-07 13:24:43 (3.77 MB/s) - ‘spark-3.4.1-bin-hadoop3.tgz’ saved [388341449/388341449]\n",
            "\n",
            "-rw-r--r-- 1 root root 371M Jun 19  2023 spark-3.4.1-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install findspark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "uzFl5pMm8ZEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "# Set environment paths\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "# Initialize findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"FlightDelayPrediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "DNIKfiyB8lvR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "ebaeba14-66b1-472e-f101-03a830017ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ad27c51f7d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f65a564e5ec1:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>FlightDelayPrediction</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "delays_pd = pd.read_csv(\"https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\")\n",
        "airports_pd = pd.read_csv(\"https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\", sep=\"\\t\")\n",
        "\n",
        "# Preview\n",
        "delays_pd.head(), airports_pd.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_rnp45i87X4",
        "outputId": "5eac98b9-45f4-4f14-d23a-33bd0ba3f9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(      date  delay  distance origin destination\n",
              " 0  1011245      6       602    ABE         ATL\n",
              " 1  1020600     -8       369    ABE         DTW\n",
              " 2  1021245     -2       602    ABE         ATL\n",
              " 3  1020605     -4       602    ABE         ATL\n",
              " 4  1031245     -4       602    ABE         ATL,\n",
              "          City State Country IATA\n",
              " 0  Abbotsford    BC  Canada  YXX\n",
              " 1    Aberdeen    SD     USA  ABR\n",
              " 2     Abilene    TX     USA  ABI\n",
              " 3       Akron    OH     USA  CAK\n",
              " 4     Alamosa    CO     USA  ALS)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Spark DataFrames\n",
        "delays_df = spark.createDataFrame(delays_pd)\n",
        "airports_df = spark.createDataFrame(airports_pd)\n",
        "\n",
        "# Check schema\n",
        "delays_df.printSchema()\n",
        "airports_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWiAemMH9HkM",
        "outputId": "a15c0e98-3527-4608-dee0-6d586908debb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: long (nullable = true)\n",
            " |-- delay: long (nullable = true)\n",
            " |-- distance: long (nullable = true)\n",
            " |-- origin: string (nullable = true)\n",
            " |-- destination: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- IATA: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for SEA and SFO\n",
        "filtered_df = delays_df.filter(delays_df.origin.isin(\"SEA\", \"SFO\"))\n",
        "\n",
        "# Drop nulls and limit rows for faster processing\n",
        "filtered_df = filtered_df.na.drop().limit(10000)\n",
        "\n",
        "# Show sample\n",
        "filtered_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEZLhIcX9X63",
        "outputId": "edc3eb97-0e07-408a-f538-8aed89f7d0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+------+-----------+\n",
            "|   date|delay|distance|origin|destination|\n",
            "+-------+-----+--------+------+-----------+\n",
            "|1011425|   92|    1495|   SEA|        ORD|\n",
            "|1010715|   -7|    2104|   SEA|        JFK|\n",
            "|1010830|   -5|    1442|   SEA|        DFW|\n",
            "|1012205|   -3|    2367|   SEA|        MIA|\n",
            "|1010600|   -3|    1442|   SEA|        DFW|\n",
            "+-------+-----+--------+------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Create binary label\n",
        "labeled_df = filtered_df.withColumn(\"label\", when(filtered_df.delay > 10, 1).otherwise(0))\n",
        "\n",
        "# Show sample\n",
        "labeled_df.select(\"origin\", \"destination\", \"delay\", \"label\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxBLXDqQ95V2",
        "outputId": "1764692f-5de4-4208-ac76-8d2d148d9508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-----+-----+\n",
            "|origin|destination|delay|label|\n",
            "+------+-----------+-----+-----+\n",
            "|   SEA|        ORD|   92|    1|\n",
            "|   SEA|        JFK|   -7|    0|\n",
            "|   SEA|        DFW|   -5|    0|\n",
            "|   SEA|        MIA|   -3|    0|\n",
            "|   SEA|        DFW|   -3|    0|\n",
            "+------+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Encode origin and destination\n",
        "origin_indexer = StringIndexer(inputCol=\"origin\", outputCol=\"origin_index\")\n",
        "dest_indexer = StringIndexer(inputCol=\"destination\", outputCol=\"dest_index\")\n",
        "\n",
        "# Check if distance column exists\n",
        "if \"distance\" in labeled_df.columns:\n",
        "    input_features = [\"origin_index\", \"dest_index\", \"distance\"]\n",
        "else:\n",
        "    input_features = [\"origin_index\", \"dest_index\"]\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n",
        "\n",
        "# Define model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline(stages=[origin_indexer, dest_indexer, assembler, lr])"
      ],
      "metadata": {
        "id": "K16-rO1V99iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "train_data, test_data = labeled_df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Train model\n",
        "model = pipeline.fit(train_data)"
      ],
      "metadata": {
        "id": "lIM6r2jA-tVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Show output\n",
        "predictions.select(\"origin\", \"destination\", \"delay\", \"label\", \"prediction\", \"probability\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JFUl2e1bUqQ",
        "outputId": "380e8189-bd6e-4c84-bd29-8dbcc7bf977a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-----+-----+----------+--------------------+\n",
            "|origin|destination|delay|label|prediction|         probability|\n",
            "+------+-----------+-----+-----+----------+--------------------+\n",
            "|   SEA|        DEN|    7|    0|       0.0|[0.79761836102784...|\n",
            "|   SEA|        SLC|    0|    0|       0.0|[0.80880401119766...|\n",
            "|   SEA|        ANC|   -3|    0|       0.0|[0.79680179611274...|\n",
            "|   SEA|        DFW|   -3|    0|       0.0|[0.78622802805093...|\n",
            "|   SFO|        DFW|   -3|    0|       0.0|[0.82196867460839...|\n",
            "|   SEA|        OAK|   -2|    0|       0.0|[0.81030399833031...|\n",
            "|   SEA|        ORD|   -3|    0|       0.0|[0.79009604162233...|\n",
            "|   SEA|        IAH|   13|    1|       0.0|[0.80991377166334...|\n",
            "|   SEA|        SAN|    2|    0|       0.0|[0.81259286186706...|\n",
            "|   SEA|        LAX|   -2|    0|       0.0|[0.79103634987026...|\n",
            "+------+-----------+-----+-----+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Evaluate using AUC\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Updated AUC: {auc:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "if auc > 0.8:\n",
        "    print(\"✅ The model performs well at distinguishing delays.\")\n",
        "elif auc > 0.6:\n",
        "    print(\"🟡 The model has moderate performance. Consider adding features.\")\n",
        "else:\n",
        "    print(\"🔴 The model performs poorly. Needs improvement with better features or models.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsEwennqbXLQ",
        "outputId": "d2052d17-0403-4c66-8639-038a55ea4d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated AUC: 0.5380\n",
            "🔴 The model performs poorly. Needs improvement with better features or models.\n"
          ]
        }
      ]
    }
  ]
}