# -*- coding: utf-8 -*-
"""Lab2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pe5JHwgl7l9Vw31uAzcdQjM98701oJKS
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, sum as _sum
from google.colab import files, drive

# Upload the file directly to Colab
uploaded = files.upload()

# Initialize Spark Session
spark = SparkSession.builder.appName("PartitionStrategies").getOrCreate()

# Load the CSV data (adjust filename if needed)
df = spark.read.csv("owid-covid-data.csv", header=True, inferSchema=True)

# 1. Hash Partitioning by 'location'
hash_partitioned_df = df.repartition(4, col("location"))

# Transformation Pipeline for Hash Partitioning
# - Filter: Data for 'Philippines'
# - Summarize: Average new cases
# - Sort: By date
philippines_summary = (
    hash_partitioned_df.filter(col("location") == "Philippines")
    .groupBy("location")
    .agg(avg("new_cases").alias("avg_new_cases"))
    .orderBy("avg_new_cases", ascending=False)
)

# 2. Range Partitioning by 'date'
range_partitioned_df = df.repartitionByRange(4, col("date"))

# Transformation Pipeline for Range Partitioning
# - Filter: Dates after '2021-01-01'
# - Summarize: Total new cases per country
# - Sort: By total cases descending
recent_cases_summary = (
    range_partitioned_df.filter(col("date") > "2021-01-01")
    .groupBy("location")
    .agg(_sum("new_cases").alias("total_new_cases"))
    .orderBy("total_new_cases", ascending=False)
)

# Show results
philippines_summary.show()
recent_cases_summary.show()

# Stop Spark Session
spark.stop()